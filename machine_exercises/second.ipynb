{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb65963a",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb363a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import math\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import einsum, rearrange\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision.models import AlexNet_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526892a",
   "metadata": {},
   "source": [
    "#### Load the weights and biases of AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c0ee8f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['features.0.weight', 'features.0.bias', 'features.3.weight', 'features.3.bias', 'features.6.weight', 'features.6.bias', 'features.8.weight', 'features.8.bias', 'features.10.weight', 'features.10.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.4.weight', 'classifier.4.bias', 'classifier.6.weight', 'classifier.6.bias'])\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = AlexNet_Weights.DEFAULT.get_state_dict()\n",
    "print(weights_and_biases.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5429bfd",
   "metadata": {},
   "source": [
    "#### Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "8698e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_collate(batch):\n",
    "    \"\"\"\n",
    "    A collation function that simply returns the batch as is.\n",
    "    We convert torch tensors to numpy arrays since np.pad doesn't work on tensors\n",
    "    \"\"\"\n",
    "    imgs, labels = zip(*batch)                   # list of (np.ndarray, int)\n",
    "    imgs = np.stack(imgs, axis=0)               # [B, 3, 224, 224] (np.float32)\n",
    "    labels = np.asarray(labels, dtype=np.int64)  # [B]\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "# implement using ImageNet\n",
    "imagenet_val = ImageNet(\n",
    "    root=\"data/ImageNet1k\",\n",
    "    split=\"val\",\n",
    "    transform=AlexNet_Weights.IMAGENET1K_V1.transforms()\n",
    ")\n",
    "\n",
    "# the dataloader automatically segregates the labels\n",
    "val_dataloader = DataLoader(\n",
    "    imagenet_val,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    collate_fn=default_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "bb1111ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 3, 224, 224)\n",
      "(128,)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in val_dataloader:\n",
    "    print(images.shape)\n",
    "    print(labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11075a2b",
   "metadata": {},
   "source": [
    "#### Define the custom Conv2d\n",
    "\n",
    "Here we just inherit from nn.Module so that it will work cleanly with pytorch even though the implementation of the computation is done by einops, einsum, and numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "552c4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMixin:\n",
    "    def __init__(self, kernel_size: int, stride: int) -> None:\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def _patch_with_stride(self, x_pad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extracts k x k patches (kernel dims) from the input array with the given stride.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input array of shape (b, c, h, w).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of shape (b, c, h/2, w/2, k, k) containing the extracted patches.\n",
    "        \"\"\"\n",
    "        windows = sliding_window_view(\n",
    "            # type: ignore\n",
    "            x_pad,\n",
    "            window_shape=(self.kernel_size, self.kernel_size),\n",
    "            axis=(-2, -1)  # type: ignore\n",
    "        )\n",
    "\n",
    "        # Stride by taking every second window in both height and width dimensions\n",
    "        return windows[:, :, ::self.stride, ::self.stride, :, :]\n",
    "\n",
    "\n",
    "class WeightsAndBiasMixin:\n",
    "    def __init__(self, weights_loc: str, bias_loc: str, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.weights_loc = weights_loc\n",
    "        self.bias_loc = bias_loc\n",
    "\n",
    "    def init_weights_and_bias(self) -> None:\n",
    "        print(f'Weights and biases: {weights_and_biases.keys()}')\n",
    "        weights = np.load(self.weights_loc)\n",
    "        bias = np.load(self.bias_loc)\n",
    "        self.weight = nn.Parameter(torch.from_numpy(weights.astype('float32')))\n",
    "        self.bias = torch.from_numpy(bias.astype('float32'))\n",
    "\n",
    "\n",
    "class CustomConv2d(WeightsAndBiasMixin, PatchMixin, nn.Module):\n",
    "    \"\"\"\n",
    "    2D Convolution layer using NumPy, Einops, and einsum.\n",
    "\n",
    "    This isn't as flexible yet because it only supports very specific shapes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "        weights_loc: str = '',\n",
    "        bias_loc: str = '',\n",
    "        # bias: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__(weights_loc, bias_loc, kernel_size, stride)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        # just make sure to match features.0.weight and features.0.bias params from AlexNet\n",
    "        # self.weight = nn.Parameter(torch.empty(\n",
    "        #     out_channels, in_channels, kernel_size, kernel_size))\n",
    "\n",
    "        # TODO: Change this\n",
    "        test_kernel = np.arange(12).reshape(3, 1, 2, 2)\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.from_numpy(test_kernel.astype('float32')))\n",
    "        # self.bias = nn.Parameter(torch.empty(out_channels)) if bias else 0\n",
    "        self.bias = np.arange(3)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Since we are only going to be using this for inference, then we can just pass.\n",
    "        We'll just fill this up when needed.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _apply_padding(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.pad(\n",
    "            x,\n",
    "            pad_width=((0, 0), (0, 0), (self.padding, self.padding),\n",
    "                       (self.padding, self.padding)),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x_pad = self._apply_padding(x)\n",
    "        patched_windows = self._patch_with_stride(x_pad)\n",
    "        pre_activation = einsum(patched_windows, self.weight,\n",
    "                                'b c w h kw kh, o c kw kh -> b o w h')\n",
    "\n",
    "        return pre_activation + self.bias[None, :, None, None]  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8bf053fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom ReLU activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(x, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9152ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMaxPool2d(PatchMixin, nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Max Pooling layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size: int, stride: int) -> None:\n",
    "        super().__init__(kernel_size, stride)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        patched_windows = self._patch_with_stride(x)\n",
    "        return np.max(patched_windows, axis=(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60188491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdaptiveAvgPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Adaptive Average Pooling layer (NumPy-based, inference only).\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple[int, int]): desired (out_h, out_w)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size: tuple[int, int]) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (np.ndarray): input of shape (b, c, h, w)\n",
    "        Returns:\n",
    "            np.ndarray: pooled output of shape (b, c, out_h, out_w)\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.shape\n",
    "        out_h, out_w = self.output_size\n",
    "\n",
    "        # compute region boundaries for adaptive pooling\n",
    "        out = np.zeros((b, c, out_h, out_w), dtype=x.dtype)\n",
    "        for i in range(out_h):\n",
    "            h_start = int(np.floor(i * h / out_h))\n",
    "            h_end = int(np.ceil((i+1) * h / out_h))\n",
    "            for j in range(out_w):\n",
    "                w_start = int(np.floor(j * w / out_w))\n",
    "                w_end = int(np.ceil((j+1) * w / out_w))\n",
    "                # (b, c, h_slice, w_slice)\n",
    "                region = x[:, :, h_start:h_end, w_start:w_end]\n",
    "                out[:, :, i, j] = region.mean(axis=(-2, -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "22d57fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 43.]]\n",
      "\n",
      "  [[140.]]\n",
      "\n",
      "  [[237.]]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(9).reshape(1, 1, 3, 3)\n",
    "conv2d = CustomConv2d(1, 1, 2, stride=2, padding=1)\n",
    "relu = CustomReLU()\n",
    "maxpool = CustomMaxPool2d(kernel_size=2, stride=1)\n",
    "with torch.no_grad():\n",
    "    y = conv2d.forward(x)\n",
    "    y = relu.forward(y)\n",
    "    y = maxpool.forward(y)\n",
    "    print(y)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
