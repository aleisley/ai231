{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb65963a",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb363a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import einsum, rearrange\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision.models import AlexNet_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526892a",
   "metadata": {},
   "source": [
    "#### Load the weights and biases of AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0ee8f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['features.0.weight', 'features.0.bias', 'features.3.weight', 'features.3.bias', 'features.6.weight', 'features.6.bias', 'features.8.weight', 'features.8.bias', 'features.10.weight', 'features.10.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.4.weight', 'classifier.4.bias', 'classifier.6.weight', 'classifier.6.bias'])\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = AlexNet_Weights.DEFAULT.get_state_dict()\n",
    "print(weights_and_biases.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5429bfd",
   "metadata": {},
   "source": [
    "#### Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8698e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_collate(batch):\n",
    "    \"\"\"\n",
    "    A collation function that simply returns the batch as is.\n",
    "    We convert torch tensors to numpy arrays since np.pad doesn't work on tensors\n",
    "    \"\"\"\n",
    "    imgs, labels = zip(*batch)                   # list of (np.ndarray, int)\n",
    "    imgs = np.stack(imgs, axis=0)               # [B, 3, 224, 224] (np.float32)\n",
    "    labels = np.asarray(labels, dtype=np.int64)  # [B]\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "# implement using ImageNet\n",
    "imagenet_val = ImageNet(\n",
    "    root=\"data/ImageNet1k\",\n",
    "    split=\"val\",\n",
    "    transform=AlexNet_Weights.IMAGENET1K_V1.transforms()\n",
    ")\n",
    "\n",
    "# the dataloader automatically segregates the labels\n",
    "val_dataloader = DataLoader(\n",
    "    imagenet_val,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    # persistent_workers=True,\n",
    "    collate_fn=default_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb1111ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "images = None\n",
    "labels = None\n",
    "for images, labels in val_dataloader:\n",
    "    images = images[0]\n",
    "    labels = labels[0]\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11075a2b",
   "metadata": {},
   "source": [
    "#### Define the custom Conv2d\n",
    "\n",
    "Here we just inherit from nn.Module so that it will work cleanly with pytorch even though the implementation of the computation is done by einops, einsum, and numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552c4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMixin:\n",
    "    def __init__(self, kernel_size: int, stride: int) -> None:\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def _patch_with_stride(self, x_pad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Extracts k x k patches (kernel dims) from the input array with the given stride.\n",
    "\n",
    "        Args:\n",
    "            x (np.ndarray): Input array of shape (b, c, h, w).\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Array of shape (b, c, h/2, w/2, k, k) containing the extracted patches.\n",
    "        \"\"\"\n",
    "        windows = sliding_window_view(\n",
    "            # type: ignore\n",
    "            x_pad,\n",
    "            window_shape=(self.kernel_size, self.kernel_size),\n",
    "            axis=(-2, -1)  # type: ignore\n",
    "        )\n",
    "\n",
    "        # Stride by taking every second window in both height and width dimensions\n",
    "        return windows[:, :, ::self.stride, ::self.stride, :, :]\n",
    "\n",
    "\n",
    "class WeightsAndBiasMixin:\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def init_weights_and_bias(self, weight_loc: str, bias_loc: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        weight = weights_and_biases[weight_loc].detach().numpy()\n",
    "        bias = weights_and_biases[bias_loc].detach().numpy()\n",
    "\n",
    "        return weight, bias\n",
    "\n",
    "\n",
    "class CustomConv2d(WeightsAndBiasMixin, PatchMixin, nn.Module):\n",
    "    \"\"\"\n",
    "    2D Convolution layer using NumPy, Einops, and einsum.\n",
    "\n",
    "    This isn't as flexible yet because it only supports very specific shapes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "        weight_loc: str = '',\n",
    "        bias_loc: str = '',\n",
    "        # bias: bool = True\n",
    "    ) -> None:\n",
    "        super().__init__(kernel_size, stride)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.weight, self.bias = self.init_weights_and_bias(\n",
    "            weight_loc, bias_loc)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"\n",
    "        Since we are only going to be using this for inference, then we can just pass.\n",
    "        We'll just fill this up when needed.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _apply_padding(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.pad(\n",
    "            x,\n",
    "            pad_width=((0, 0), (0, 0), (self.padding, self.padding),\n",
    "                       (self.padding, self.padding)),\n",
    "            mode='constant',\n",
    "            constant_values=0\n",
    "        )\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x_pad = self._apply_padding(x)\n",
    "        patched_windows = self._patch_with_stride(x_pad)\n",
    "        pre_activation = einsum(patched_windows, self.weight,\n",
    "                                'b c w h kw kh, o c kw kh -> b o w h')\n",
    "\n",
    "        return pre_activation + self.bias[None, :, None, None]  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf053fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom ReLU activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.maximum(x, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9152ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMaxPool2d(PatchMixin, nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Max Pooling layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size: int, stride: int) -> None:\n",
    "        super().__init__(kernel_size, stride)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        patched_windows = self._patch_with_stride(x)\n",
    "        return np.max(patched_windows, axis=(-2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60188491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdaptiveAvgPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Adaptive Average Pooling layer (NumPy-based, inference only).\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple[int, int]): desired (out_h, out_w)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size: tuple[int, int]) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (np.ndarray): input of shape (b, c, h, w)\n",
    "        Returns:\n",
    "            np.ndarray: pooled output of shape (b, c, out_h, out_w)\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.shape\n",
    "        out_h, out_w = self.output_size\n",
    "\n",
    "        # compute region boundaries for adaptive pooling\n",
    "        out = np.zeros((b, c, out_h, out_w), dtype=x.dtype)\n",
    "        for i in range(out_h):\n",
    "            h_start = int(np.floor(i * h / out_h))\n",
    "            h_end = int(np.ceil((i+1) * h / out_h))\n",
    "            for j in range(out_w):\n",
    "                w_start = int(np.floor(j * w / out_w))\n",
    "                w_end = int(np.ceil((j+1) * w / out_w))\n",
    "                # (b, c, h_slice, w_slice)\n",
    "                region = x[:, :, h_start:h_end, w_start:w_end]\n",
    "                out[:, :, i, j] = region.mean(axis=(-2, -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe6b05b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinopsLinear(WeightsAndBiasMixin, nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, weight_loc: str, bias_loc: str):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight, self.bias = self.init_weights_and_bias(\n",
    "            weight_loc, bias_loc)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (np.ndarray): Input array of shape (batch, in_features).\n",
    "            self.weight (np.ndarray): Weight matrix of shape (out_features, in_features).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor of shape (batch, out_features), equivalent to x @ W.T.\n",
    "        \"\"\"\n",
    "        y = einsum(x, self.weight, \"b i, o i -> b o\")\n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feb5b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 1000) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            CustomConv2d(3, 64, kernel_size=11, stride=4, padding=2,\n",
    "                         weight_loc='features.0.weight', bias_loc='features.0.bias'),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPool2d(kernel_size=3, stride=2),\n",
    "            CustomConv2d(64, 192, kernel_size=5, padding=2,\n",
    "                         weight_loc='features.3.weight', bias_loc='features.3.bias'),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPool2d(kernel_size=3, stride=2),\n",
    "            CustomConv2d(192, 384, kernel_size=3, padding=1,\n",
    "                         weight_loc='features.6.weight', bias_loc='features.6.bias'),\n",
    "            CustomReLU(),\n",
    "            CustomConv2d(384, 256, kernel_size=3, padding=1,\n",
    "                         weight_loc='features.8.weight', bias_loc='features.8.bias'),\n",
    "            CustomReLU(),\n",
    "            CustomConv2d(256, 256, kernel_size=3, padding=1,\n",
    "                         weight_loc='features.10.weight', bias_loc='features.10.bias'),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = CustomAdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            EinopsLinear(256 * 6 * 6, 4096,\n",
    "                         weight_loc='classifier.1.weight', bias_loc='classifier.1.bias'),\n",
    "            # nn.ReLU(inplace=True),\n",
    "            CustomReLU(),\n",
    "            # nn.Dropout(),\n",
    "            EinopsLinear(4096, 4096,\n",
    "                         weight_loc='classifier.4.weight', bias_loc='classifier.4.bias'),\n",
    "            CustomReLU(),\n",
    "            # nn.Dropout(),\n",
    "            EinopsLinear(4096, num_classes,\n",
    "                         weight_loc='classifier.6.weight', bias_loc='classifier.6.bias'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(torch.from_numpy(x), 1).numpy()  # type: ignore\n",
    "        x = self.classifier(x)\n",
    "        print('finished one')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b532ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "model = AlexNet()\n",
    "model.eval()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "for images, labels in val_dataloader:  # assume your dataloader gives numpy arrays\n",
    "    # make sure they're numpy\n",
    "    if isinstance(images, torch.Tensor):\n",
    "        images = images.numpy()\n",
    "    if isinstance(labels, torch.Tensor):\n",
    "        labels = labels.numpy()\n",
    "\n",
    "    # your custom forward returns np.ndarray (b, num_classes)\n",
    "    outputs = model.forward(images)\n",
    "\n",
    "    # predicted class indices\n",
    "    predicted = np.argmax(outputs, axis=1)\n",
    "\n",
    "    total += labels.shape[0]\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai231",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
