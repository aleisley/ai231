{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82811892",
   "metadata": {},
   "source": [
    "### ME2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb65963a",
   "metadata": {},
   "source": [
    "#### Import necessary packages\n",
    "\n",
    "This notebook demonstrates custom deep learning inference using NumPy for portability and clarity.\n",
    "\n",
    "- **NumPy** is the core array library we use to implement convolution, pooling, activation, and linear layers from scratch.\n",
    "- We still rely on PyTorch / torchvision for data loading and to access pretrained AlexNet weights.\n",
    "- Supporting packages (einops for concise tensor algebra, tqdm for progress reporting) round out the tooling.\n",
    "\n",
    "The imports below are grouped to highlight standard libraries, third-party utilities, and NumPy stride tricks used for patch extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb363a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import einsum, rearrange\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision.models import AlexNet_Weights\n",
    "\n",
    "# NumPy imports\n",
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93abbf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profiling utilities: decorator to time forward methods\n",
    "import time\n",
    "from functools import wraps\n",
    "from collections import defaultdict\n",
    "\n",
    "PROFILE_EVENTS = []  # list of dicts: {\"name\": str, \"elapsed\": float}\n",
    "\n",
    "\n",
    "def profile_forward(name: str | None = None):\n",
    "    \"\"\"Decorator to measure execution time of forward methods.\n",
    "\n",
    "    Args:\n",
    "        name: Optional override name. If None attempts to derive from self.__class__.__name__.\n",
    "    \"\"\"\n",
    "    def outer(fn):\n",
    "        @wraps(fn)\n",
    "        def inner(self, *args, **kwargs):\n",
    "            label = name or self.__class__.__name__\n",
    "            start = time.perf_counter()\n",
    "            out = fn(self, *args, **kwargs)\n",
    "            end = time.perf_counter()\n",
    "            PROFILE_EVENTS.append({\"name\": label, \"elapsed\": end - start})\n",
    "            print(f'Name: {label}, Time: {(end - start)*1000:.3f} ms')\n",
    "            return out\n",
    "        return inner\n",
    "    return outer\n",
    "\n",
    "\n",
    "def profile_summary(sort: str = \"total\", top_n: int | None = None):\n",
    "    \"\"\"Print aggregated profiling results.\n",
    "\n",
    "    Args:\n",
    "        sort: One of {\"total\", \"mean\", \"calls\", \"max\"} to control ordering.\n",
    "        top_n: If provided, limit output rows.\n",
    "    \"\"\"\n",
    "    if not PROFILE_EVENTS:\n",
    "        print(\"No profiling data collected.\")\n",
    "        return\n",
    "    agg = defaultdict(lambda: {\"total\": 0.0, \"calls\": 0, \"max\": 0.0})\n",
    "    for ev in PROFILE_EVENTS:\n",
    "        a = agg[ev[\"name\"]]\n",
    "        a[\"total\"] += ev[\"elapsed\"]\n",
    "        a[\"calls\"] += 1\n",
    "        if ev[\"elapsed\"] > a[\"max\"]:\n",
    "            a[\"max\"] = ev[\"elapsed\"]\n",
    "    rows = []\n",
    "    for name, stats in agg.items():\n",
    "        mean = stats[\"total\"] / stats[\"calls\"]\n",
    "        rows.append({\"name\": name, **stats, \"mean\": mean})\n",
    "    rows.sort(key=lambda r: r[sort], reverse=True)\n",
    "    if top_n is not None:\n",
    "        rows = rows[:top_n]\n",
    "    header = f\"{'Layer':<22} {'Calls':>5} {'Total(ms)':>12} {'Mean(ms)':>12} {'Max(ms)':>12}\"\n",
    "    print(header)\n",
    "    print('-' * len(header))\n",
    "    for r in rows:\n",
    "        print(f\"{r['name']:<22} {r['calls']:>5} {r['total']*1000:>12.3f} {r['mean']*1000:>12.3f} {r['max']*1000:>12.3f}\")\n",
    "    total_time = sum(r['total'] for r in rows)\n",
    "    print('-' * len(header))\n",
    "    print(\n",
    "        f\"Total profiled time: {total_time*1000:.2f} ms across {len(PROFILE_EVENTS)} calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526892a",
   "metadata": {},
   "source": [
    "#### Load the weights and biases of AlexNet\n",
    "\n",
    "In this section, we extract the pretrained weights and biases from torchvision's AlexNet model.\n",
    "\n",
    "- The weights and biases are loaded using the default configuration from `AlexNet_Weights`.\n",
    "- These parameters are stored in a dictionary and printed to verify the available keys.\n",
    "- Our custom layers will use these pretrained values for inference, ensuring the model matches the original AlexNet architecture.\n",
    "\n",
    "This step is essential for initializing all custom layers with correct parameters before running inference on the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0ee8f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['features.0.weight', 'features.0.bias', 'features.3.weight', 'features.3.bias', 'features.6.weight', 'features.6.bias', 'features.8.weight', 'features.8.bias', 'features.10.weight', 'features.10.bias', 'classifier.1.weight', 'classifier.1.bias', 'classifier.4.weight', 'classifier.4.bias', 'classifier.6.weight', 'classifier.6.bias'])\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = AlexNet_Weights.DEFAULT.get_state_dict()\n",
    "print(weights_and_biases.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5429bfd",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "\n",
    "In this section, we load the ImageNet validation dataset and prepare it for inference with our custom NumPy-based AlexNet implementation.\n",
    "\n",
    "- We use torchvision's `ImageNet` class to access the validation split, applying the standard AlexNet preprocessing transforms.\n",
    "- The DataLoader batches images; our custom `default_collate` converts tensors directly to NumPy arrays.\n",
    "- Keeping everything in NumPy simplifies the educational focus (no device transfers or GPU specifics).\n",
    "\n",
    "This setup enables end-to-end inference using only NumPy for tensor operations alongside pretrained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8698e65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def default_collate(batch):\n",
    "    \"\"\"\n",
    "    Collate function converting incoming PyTorch tensors to NumPy arrays.\n",
    "\n",
    "    We convert tensors early so all subsequent custom layers operate purely on\n",
    "    NumPy arrays (no device transfers needed).\n",
    "    \"\"\"\n",
    "    imgs, labels = zip(*batch)  # imgs: tuple[torch.Tensor], labels: tuple[int]\n",
    "    imgs = [np.asarray(img.numpy(), dtype=np.float32) for img in imgs]\n",
    "    imgs = np.stack(imgs, axis=0)                 # [B,3,224,224]\n",
    "    labels = np.asarray(labels, dtype=np.int64)    # [B]\n",
    "    return imgs, labels\n",
    "\n",
    "\n",
    "# implement using ImageNet\n",
    "imagenet_val = ImageNet(\n",
    "    root=\"data/ImageNet1k\",\n",
    "    split=\"val\",\n",
    "    transform=AlexNet_Weights.IMAGENET1K_V1.transforms()\n",
    ")\n",
    "\n",
    "# the dataloader automatically segregates the labels\n",
    "# val_dataloader = DataLoader(\n",
    "#     imagenet_val,\n",
    "#     batch_size=512,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0,\n",
    "#     collate_fn=default_collate,\n",
    "# )\n",
    "val_dataloader = DataLoader(\n",
    "    imagenet_val,\n",
    "    batch_size=128,\n",
    "    num_workers=12,\n",
    "    prefetch_factor=4,\n",
    "    persistent_workers=True,\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=default_collate\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11075a2b",
   "metadata": {},
   "source": [
    "#### Define the custom Conv2d\n",
    "\n",
    "We implement a minimal convolution by extracting sliding kÃ—k patches with `sliding_window_view` and contracting them with the weight tensor using `einsum`. This mirrors PyTorch's Conv2d behavior for stride and padding in a clear, NumPy-only form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "552c4b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMixin:\n",
    "    \"\"\"Mixin for extracting patches from input arrays with a given kernel size and stride.\n",
    "    Used for convolution and pooling operations on NumPy arrays.\"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size: int, stride: int) -> None:\n",
    "        \"\"\"Initialize patch extraction parameters.\n",
    "        Args:\n",
    "            kernel_size (int): Size of the square kernel.\n",
    "            stride (int): Stride for patch extraction.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "    def _patch_with_stride(self, x_pad: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Extract k x k patches from the input array with the given stride.\n",
    "        Args:\n",
    "            x_pad (np.ndarray): Input array of shape (b, c, h, w).\n",
    "        Returns:\n",
    "            np.ndarray: Array of shape (b, c, h/stride, w/stride, k, k) containing the extracted patches.\n",
    "        \"\"\"\n",
    "        windows = sliding_window_view(  # type: ignore\n",
    "            x_pad,\n",
    "            window_shape=(self.kernel_size, self.kernel_size),\n",
    "            axis=(-2, -1)  # type: ignore\n",
    "        )\n",
    "        return windows[:, :, ::self.stride, ::self.stride, :, :]\n",
    "\n",
    "\n",
    "class WeightsAndBiasMixin:\n",
    "    \"\"\"Mixin for loading pretrained weights and biases from a state dict as NumPy arrays.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs) -> None:\n",
    "        \"\"\"Initialize the mixin (calls super).\"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def init_weights_and_bias(self, weight_loc: str, bias_loc: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Load weights and biases from the state dict.\n",
    "        Args:\n",
    "            weight_loc (str): Key for weights in the state dict.\n",
    "            bias_loc (str): Key for biases in the state dict.\n",
    "        Returns:\n",
    "            Tuple[np.ndarray, np.ndarray]: NumPy arrays for weights and biases.\n",
    "        \"\"\"\n",
    "        weight = weights_and_biases[weight_loc].detach().cpu().numpy()\n",
    "        bias = weights_and_biases[bias_loc].detach().cpu().numpy()\n",
    "        return weight, bias\n",
    "\n",
    "\n",
    "class CustomConv2d(WeightsAndBiasMixin, PatchMixin, nn.Module):\n",
    "    \"\"\"\n",
    "    Custom 2D Convolution layer using NumPy and einsum.\n",
    "    Performs convolution on NumPy arrays.\n",
    "    Limited shape flexibility for demonstration/inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int = 1,\n",
    "        padding: int = 0,\n",
    "        weight_loc: str = '',\n",
    "        bias_loc: str = '',\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the convolution layer with parameters and pretrained weights/biases.\n",
    "        Args:\n",
    "            in_channels (int): Number of input channels.\n",
    "            out_channels (int): Number of output channels.\n",
    "            kernel_size (int): Size of the convolution kernel.\n",
    "            stride (int, optional): Stride for convolution. Defaults to 1.\n",
    "            padding (int, optional): Padding for input. Defaults to 0.\n",
    "            weight_loc (str, optional): Key for weights in state dict.\n",
    "            bias_loc (str, optional): Key for biases in state dict.\n",
    "        \"\"\"\n",
    "        super().__init__(kernel_size, stride)\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.weight, self.bias = self.init_weights_and_bias(\n",
    "            weight_loc, bias_loc)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self) -> None:\n",
    "        \"\"\"No-op for pretrained weights. Provided for API compatibility.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _apply_padding(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply zero padding to the input array if required.\n",
    "        Args:\n",
    "            x (np.ndarray): Input array of shape (b, c, h, w).\n",
    "        Returns:\n",
    "            np.ndarray: Padded input array.\n",
    "        \"\"\"\n",
    "        if self.padding == 0:\n",
    "            return x\n",
    "        return np.pad(\n",
    "            x,\n",
    "            pad_width=((0, 0), (0, 0), (self.padding, self.padding),\n",
    "                       (self.padding, self.padding)),\n",
    "            mode='constant',\n",
    "            constant_values=0,\n",
    "        )\n",
    "\n",
    "    @profile_forward()\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Perform the forward pass of the convolution layer.\n",
    "        Args:\n",
    "            x (np.ndarray): Input array of shape (b, c, h, w).\n",
    "        Returns:\n",
    "            np.ndarray: Output array after convolution and bias addition.\n",
    "        \"\"\"\n",
    "        x_pad = self._apply_padding(x)\n",
    "        patched_windows = self._patch_with_stride(x_pad)\n",
    "        pre_activation = einsum(\n",
    "            patched_windows, self.weight, 'b c w h kw kh, o c kw kh -> b o w h')\n",
    "        return pre_activation + self.bias[None, :, None, None]  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e56b1",
   "metadata": {},
   "source": [
    "#### Custom ReLU\n",
    "\n",
    "A straightforward NumPy implementation of ReLU using `np.maximum`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bf053fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomReLU(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom ReLU activation layer using NumPy.\n",
    "\n",
    "    Applies the Rectified Linear Unit (ReLU) function: f(x)=max(0,x) element-wise.\n",
    "    \"\"\"\n",
    "\n",
    "    @profile_forward()\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply ReLU to the input.\n",
    "        Args:\n",
    "            x (np.ndarray): Input tensor of any shape.\n",
    "        Returns:\n",
    "            np.ndarray: Same shape as input with negatives zeroed.\n",
    "        \"\"\"\n",
    "        return np.maximum(x, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e33de1",
   "metadata": {},
   "source": [
    "#### Custom MaxPool2d\n",
    "\n",
    "Implemented via the same patch extraction utility as convolution, followed by a reduction (`np.max`) over the spatial kernel dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9152ca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMaxPool2d(PatchMixin, nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Max Pooling layer using NumPy.\n",
    "\n",
    "    Extracts spatial patches then takes the maximum over each patch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size: int, stride: int) -> None:\n",
    "        super().__init__(kernel_size, stride)\n",
    "\n",
    "    @profile_forward()\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply max pooling.\n",
    "        Args:\n",
    "            x (np.ndarray): Input of shape (batch, channels, height, width).\n",
    "        Returns:\n",
    "            np.ndarray: Pooled output.\n",
    "        \"\"\"\n",
    "        patched_windows = self._patch_with_stride(x)\n",
    "        return np.max(patched_windows, axis=(-2, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f39ecc6",
   "metadata": {},
   "source": [
    "#### Custom Adaptive AvgPool2d\n",
    "\n",
    "Reduces arbitrary spatial dimensions to a fixed target by averaging over partitioned regions computed with integer floor/ceil boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60188491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomAdaptiveAvgPool2d(nn.Module):\n",
    "    \"\"\"\n",
    "    Adaptive Average Pooling layer implemented with NumPy.\n",
    "\n",
    "    Reduces spatial dimensions to a target (H_out, W_out) by averaging over\n",
    "    variable-sized input regions computed via integer partitioning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size: tuple[int, int]) -> None:\n",
    "        super().__init__()\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            self.output_size = output_size\n",
    "\n",
    "    @profile_forward()\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Apply adaptive average pooling.\n",
    "        Args:\n",
    "            x (np.ndarray): Input of shape (batch, channels, height, width).\n",
    "        Returns:\n",
    "            np.ndarray: Output of shape (batch, channels, H_out, W_out).\n",
    "        \"\"\"\n",
    "        b, c, h, w = x.shape\n",
    "        out_h, out_w = self.output_size\n",
    "        out = np.zeros((b, c, out_h, out_w), dtype=x.dtype)\n",
    "        for i in range(out_h):\n",
    "            h_start = int(np.floor(i * h / out_h))\n",
    "            h_end = int(np.ceil((i + 1) * h / out_h))\n",
    "            for j in range(out_w):\n",
    "                w_start = int(np.floor(j * w / out_w))\n",
    "                w_end = int(np.ceil((j + 1) * w / out_w))\n",
    "                region = x[:, :, h_start:h_end, w_start:w_end]\n",
    "                out[:, :, i, j] = region.mean(axis=(-2, -1))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c97264d",
   "metadata": {},
   "source": [
    "#### Custom Linear Module\n",
    "\n",
    "Uses `einsum` to express the matrix multiply `x W^T` cleanly, then adds the bias. Operates purely on NumPy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe6b05b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EinopsLinear(WeightsAndBiasMixin, nn.Module):\n",
    "    \"\"\"\n",
    "    Custom Linear (fully connected) layer using NumPy and einops.\n",
    "\n",
    "    Performs y = x W^T + b via einsum for clarity.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features: int, out_features: int, weight_loc: str, bias_loc: str):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight, self.bias = self.init_weights_and_bias(\n",
    "            weight_loc, bias_loc)\n",
    "\n",
    "    @profile_forward()\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass.\n",
    "        Args:\n",
    "            x (np.ndarray): Shape (batch, in_features).\n",
    "        Returns:\n",
    "            np.ndarray: Shape (batch, out_features).\n",
    "        \"\"\"\n",
    "        y = einsum(x, self.weight, \"b i, o i -> b o\")\n",
    "        if self.bias is not None:\n",
    "            y = y + self.bias\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd790db",
   "metadata": {},
   "source": [
    "#### AlexNet Class Implementation\n",
    "\n",
    "Assembles the feature extractor, adaptive pooling, and classifier blocks using the custom NumPy-based layers defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "feb5b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom AlexNet implementation using NumPy for educational inference.\n",
    "\n",
    "    Replicates the original AlexNet architecture with all major layers\n",
    "    (convolution, pooling, linear, activation) implemented from scratch\n",
    "    operating on NumPy arrays. Pretrained weights are loaded from the\n",
    "    torchvision reference model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes: int = 1000) -> None:\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            CustomConv2d(3, 64, kernel_size=11, stride=4, padding=2,\n",
    "                         weight_loc='features.0.weight', bias_loc='features.0.bias'),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPool2d(kernel_size=3, stride=2),\n",
    "            CustomConv2d(64, 192, kernel_size=5, padding=2,\n",
    "                         weight_loc='features.3.weight', bias_loc='features.3.bias'),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPool2d(kernel_size=3, stride=2),\n",
    "            CustomConv2d(192, 384, kernel_size=3, padding=1,\n",
    "                         weight_loc='features.6.weight', bias_loc='features.6.bias'),\n",
    "            CustomReLU(),\n",
    "            CustomConv2d(384, 256, kernel_size=3, padding=1,\n",
    "                         weight_loc='features.8.weight', bias_loc='features.8.bias'),\n",
    "            CustomReLU(),\n",
    "            CustomConv2d(256, 256, kernel_size=3, padding=1,\n",
    "                         weight_loc='features.10.weight', bias_loc='features.10.bias'),\n",
    "            CustomReLU(),\n",
    "            CustomMaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = CustomAdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            EinopsLinear(256 * 6 * 6, 4096, weight_loc='classifier.1.weight',\n",
    "                         bias_loc='classifier.1.bias'),\n",
    "            CustomReLU(),\n",
    "            EinopsLinear(4096, 4096, weight_loc='classifier.4.weight',\n",
    "                         bias_loc='classifier.4.bias'),\n",
    "            CustomReLU(),\n",
    "            EinopsLinear(4096, num_classes, weight_loc='classifier.6.weight',\n",
    "                         bias_loc='classifier.6.bias'),\n",
    "        )\n",
    "\n",
    "    @profile_forward(\"AlexNet.forward\")\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass.\n",
    "        Args:\n",
    "            x (np.ndarray): (batch, 3, 224, 224)\n",
    "        Returns:\n",
    "            np.ndarray: (batch, num_classes) class scores.\n",
    "        \"\"\"\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        b = x.shape[0]\n",
    "        x = x.reshape(b, -1)  # flatten\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec05e96",
   "metadata": {},
   "source": [
    "#### Inference\n",
    "\n",
    "Run a forward pass over the validation set, accumulating accuracy using NumPy operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ebd56",
   "metadata": {},
   "source": [
    "#### Profiling Usage\n",
    "\n",
    "All forward methods are wrapped with a lightweight timing decorator storing per-call events in `PROFILE_EVENTS`.\n",
    "\n",
    "After running inference you can view an aggregate table:\n",
    "\n",
    "```python\n",
    "profile_summary()                 # default sort by total time\n",
    "profile_summary(sort=\"mean\")      # sort by mean per call\n",
    "profile_summary(sort=\"max\")       # highlight worst single call\n",
    "profile_summary(top_n=5)          # show only top 5 layers\n",
    "```\n",
    "\n",
    "Columns:\n",
    "\n",
    "- Total(ms): cumulative time across calls\n",
    "- Mean(ms): average per invocation\n",
    "- Max(ms): slowest single invocation\n",
    "- Calls: number of times that layer's forward executed\n",
    "\n",
    "To reset profiling data between experiments:\n",
    "\n",
    "```python\n",
    "PROFILE_EVENTS.clear()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32b532ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/391 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: CustomConv2d, Time: 4006.158 ms\n",
      "Name: CustomReLU, Time: 10.769 ms\n",
      "Name: CustomMaxPool2d, Time: 428.962 ms\n",
      "Name: CustomMaxPool2d, Time: 428.962 ms\n",
      "Name: CustomConv2d, Time: 27425.579 ms\n",
      "Name: CustomReLU, Time: 7.916 ms\n",
      "Name: CustomConv2d, Time: 27425.579 ms\n",
      "Name: CustomReLU, Time: 7.916 ms\n",
      "Name: CustomMaxPool2d, Time: 305.894 ms\n",
      "Name: CustomMaxPool2d, Time: 305.894 ms\n",
      "Name: CustomConv2d, Time: 25686.951 ms\n",
      "Name: CustomReLU, Time: 2.128 ms\n",
      "Name: CustomConv2d, Time: 25686.951 ms\n",
      "Name: CustomReLU, Time: 2.128 ms\n",
      "Name: CustomConv2d, Time: 33992.972 ms\n",
      "Name: CustomReLU, Time: 1.467 ms\n",
      "Name: CustomConv2d, Time: 33992.972 ms\n",
      "Name: CustomReLU, Time: 1.467 ms\n",
      "Name: CustomConv2d, Time: 22696.935 ms\n",
      "Name: CustomReLU, Time: 1.425 ms\n",
      "Name: CustomMaxPool2d, Time: 92.475 ms\n",
      "Name: CustomAdaptiveAvgPool2d, Time: 2.481 ms\n",
      "Name: CustomConv2d, Time: 22696.935 ms\n",
      "Name: CustomReLU, Time: 1.425 ms\n",
      "Name: CustomMaxPool2d, Time: 92.475 ms\n",
      "Name: CustomAdaptiveAvgPool2d, Time: 2.481 ms\n",
      "Name: EinopsLinear, Time: 802.157 ms\n",
      "Name: CustomReLU, Time: 0.157 ms\n",
      "Name: EinopsLinear, Time: 802.157 ms\n",
      "Name: CustomReLU, Time: 0.157 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 1/391 [02:00<13:04:17, 120.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: EinopsLinear, Time: 222.093 ms\n",
      "Name: CustomReLU, Time: 0.152 ms\n",
      "Name: EinopsLinear, Time: 49.427 ms\n",
      "Name: AlexNet.forward, Time: 115738.870 ms\n",
      "Running Acc: 0.7422\n",
      "Name: CustomConv2d, Time: 3832.251 ms\n",
      "Name: CustomReLU, Time: 10.684 ms\n",
      "Name: CustomConv2d, Time: 3832.251 ms\n",
      "Name: CustomReLU, Time: 10.684 ms\n",
      "Name: CustomMaxPool2d, Time: 429.246 ms\n",
      "Name: CustomMaxPool2d, Time: 429.246 ms\n",
      "Name: CustomConv2d, Time: 27857.744 ms\n",
      "Name: CustomReLU, Time: 7.822 ms\n",
      "Name: CustomConv2d, Time: 27857.744 ms\n",
      "Name: CustomReLU, Time: 7.822 ms\n",
      "Name: CustomMaxPool2d, Time: 305.207 ms\n",
      "Name: CustomMaxPool2d, Time: 305.207 ms\n",
      "Name: CustomConv2d, Time: 26001.330 ms\n",
      "Name: CustomReLU, Time: 2.234 ms\n",
      "Name: CustomConv2d, Time: 26001.330 ms\n",
      "Name: CustomReLU, Time: 2.234 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 1/391 [03:33<23:10:36, 213.94s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     11\u001b[39m     total_batches = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(val_dataloader, total=total_batches, desc=\u001b[33m\"\u001b[39m\u001b[33mEvaluating\u001b[39m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# images, labels already NumPy from collate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (b, num_classes) np.ndarray\u001b[39;00m\n\u001b[32m     16\u001b[39m     predicted = np.argmax(outputs, axis=\u001b[32m1\u001b[39m)\n\u001b[32m     17\u001b[39m     total += \u001b[38;5;28mint\u001b[39m(labels.shape[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mprofile_forward.<locals>.outer.<locals>.inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m label = name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n\u001b[32m     19\u001b[39m start = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m out = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m end = time.perf_counter()\n\u001b[32m     22\u001b[39m PROFILE_EVENTS.append({\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: label, \u001b[33m\"\u001b[39m\u001b[33melapsed\u001b[39m\u001b[33m\"\u001b[39m: end - start})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mAlexNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;129m@profile_forward\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAlexNet.forward\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: np.ndarray) -> np.ndarray:\n\u001b[32m     47\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Forward pass.\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m        x (np.ndarray): (batch, 3, 224, 224)\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m        np.ndarray: (batch, num_classes) class scores.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.avgpool(x)\n\u001b[32m     55\u001b[39m     b = x.shape[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:244\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    243\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mprofile_forward.<locals>.outer.<locals>.inner\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     18\u001b[39m label = name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\n\u001b[32m     19\u001b[39m start = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m out = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m end = time.perf_counter()\n\u001b[32m     22\u001b[39m PROFILE_EVENTS.append({\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: label, \u001b[33m\"\u001b[39m\u001b[33melapsed\u001b[39m\u001b[33m\"\u001b[39m: end - start})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mCustomConv2d.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    117\u001b[39m x_pad = \u001b[38;5;28mself\u001b[39m._apply_padding(x)\n\u001b[32m    118\u001b[39m patched_windows = \u001b[38;5;28mself\u001b[39m._patch_with_stride(x_pad)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m pre_activation = \u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpatched_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mb c w h kw kh, o c kw kh -> b o w h\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pre_activation + \u001b[38;5;28mself\u001b[39m.bias[\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/einops/einops.py:916\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(*tensors_and_pattern)\u001b[39m\n\u001b[32m    914\u001b[39m tensors = tensors_and_pattern[:-\u001b[32m1\u001b[39m]\n\u001b[32m    915\u001b[39m pattern = _compactify_pattern_for_einsum(pattern)\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/einops/_backends.py:193\u001b[39m, in \u001b[36mNumpyBackend.einsum\u001b[39m\u001b[34m(self, pattern, *x)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34meinsum\u001b[39m(\u001b[38;5;28mself\u001b[39m, pattern, *x):\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/mengai/ai231/.venv/lib/python3.12/site-packages/numpy/_core/einsumfunc.py:1423\u001b[39m, in \u001b[36meinsum\u001b[39m\u001b[34m(out, optimize, *operands, **kwargs)\u001b[39m\n\u001b[32m   1421\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m specified_out:\n\u001b[32m   1422\u001b[39m         kwargs[\u001b[33m'\u001b[39m\u001b[33mout\u001b[39m\u001b[33m'\u001b[39m] = out\n\u001b[32m-> \u001b[39m\u001b[32m1423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mc_einsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43moperands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[38;5;66;03m# Check the kwargs to avoid a more cryptic error later, without having to\u001b[39;00m\n\u001b[32m   1426\u001b[39m \u001b[38;5;66;03m# repeat default values here\u001b[39;00m\n\u001b[32m   1427\u001b[39m valid_einsum_kwargs = [\u001b[33m'\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33morder\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mcasting\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "model = AlexNet()\n",
    "model.eval()\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "\n",
    "# Estimate total batches for progress bar length\n",
    "try:\n",
    "    total_batches = len(val_dataloader)\n",
    "except TypeError:\n",
    "    total_batches = None\n",
    "\n",
    "for images, labels in tqdm(val_dataloader, total=total_batches, desc=\"Evaluating\", leave=True):\n",
    "    # images, labels already NumPy from collate\n",
    "    outputs = model.forward(images)  # (b, num_classes) np.ndarray\n",
    "    predicted = np.argmax(outputs, axis=1)\n",
    "    total += int(labels.shape[0])\n",
    "    correct += int((predicted == labels).sum())\n",
    "    running_acc = correct / total if total else 0.0\n",
    "    tqdm.write(f\"Running Acc: {running_acc:.4f}\")\n",
    "\n",
    "accuracy = correct / total if total else 0.0\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai231",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
