{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e6da44",
   "metadata": {},
   "source": [
    "### ME3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de98173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import lightning as L\n",
    "import torch\n",
    "from einops import einsum\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import ResNet18_Weights, resnet18, ResNet\n",
    "from torch import nn\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torch.nn.utils import prune\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# set precision for matmuls\n",
    "torch.set_float32_matmul_precision(\"medium\")   # or \"medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ac2ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'bn1.running_mean', 'bn1.running_var', 'bn1.weight', 'bn1.bias', 'layer1.0.conv1.weight', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.conv2.weight', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.1.conv1.weight', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.conv2.weight', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer2.0.conv1.weight', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.conv2.weight', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.1.conv1.weight', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.conv2.weight', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer3.0.conv1.weight', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.conv2.weight', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.1.conv1.weight', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.conv2.weight', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer4.0.conv1.weight', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.conv2.weight', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.1.conv1.weight', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.conv2.weight', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = ResNet18_Weights.DEFAULT.get_state_dict()\n",
    "print(weights_and_biases.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a819b28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0185, -0.0705, -0.0518,  ..., -0.0390,  0.1735, -0.0410],\n",
       "        [-0.0818, -0.0944,  0.0174,  ...,  0.2028, -0.0248,  0.0372],\n",
       "        [-0.0332, -0.0566, -0.0242,  ..., -0.0344, -0.0227,  0.0197],\n",
       "        ...,\n",
       "        [-0.0103,  0.0033, -0.0359,  ..., -0.0279, -0.0115,  0.0128],\n",
       "        [-0.0359, -0.0353, -0.0296,  ..., -0.0330, -0.0110, -0.0513],\n",
       "        [ 0.0021, -0.0248, -0.0829,  ...,  0.0417, -0.0500,  0.0663]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_and_biases['fc.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca15c0c",
   "metadata": {},
   "source": [
    "#### Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9617017",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "# Build the full training set once, then carve out a validation split\n",
    "full_train = CIFAR10(\n",
    "    root=\"data/CIFAR10\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(full_train, [45000, 5000], generator=generator)\n",
    "\n",
    "cifar10_test = CIFAR10(\n",
    "    root=\"data/CIFAR10\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar10_test,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b6831",
   "metadata": {},
   "source": [
    "#### Initialize the model and replace head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35adece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as L\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "class LitResnet18(L.LightningModule):\n",
    "    def __init__(self, num_classes: int = 10, lr: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.model.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        # freeze backbone\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if not name.startswith(\"fc\"):\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return loss, preds, y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self._shared_step(batch, batch_idx)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"train_acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self._shared_step(batch, batch_idx)\n",
    "        self.val_acc.update(preds, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self._shared_step(batch, batch_idx)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"test_acc\", self.test_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        trainable = (p for p in self.parameters() if p.requires_grad)\n",
    "        optimizer = torch.optim.SGD(\n",
    "            trainable, lr=self.hparams.lr, momentum=0.9, weight_decay=1e-4\n",
    "        )\n",
    "        scheduler = MultiStepLR(optimizer, milestones=[30, 60], gamma=0.1)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ed450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_dir: str = \"data/CIFAR10\", \n",
    "            batch_size: int = 256, \n",
    "            num_workers: int = 0\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "    def setup(self, stage: str | None = None):\n",
    "        self.generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "        if stage in (None, 'fit', 'validate'):\n",
    "            cifar10_full = CIFAR10(\n",
    "                root=self.data_dir,\n",
    "                train=True,\n",
    "                transform=self.transform,\n",
    "                download=True,\n",
    "            )\n",
    "            self.cifar10_train, self.cifar10_val = random_split(\n",
    "                cifar10_full, [45000, 5000], generator=self.generator\n",
    "            )\n",
    "        \n",
    "        if stage in (None, \"test\", \"predict\"):\n",
    "            self.cifar10_test = CIFAR10(\n",
    "                root=self.data_dir,\n",
    "                train=False,\n",
    "                transform=self.transform,\n",
    "                download=True,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cifar10_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cifar10_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cifar10_test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=self.num_workers > 0,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fe4aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 11.2 M | train\n",
      "1 | loss      | CrossEntropyLoss   | 0      | train\n",
      "2 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "3 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "4 | train_acc | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "72        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/ai231/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/ai231/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 175/175 [01:55<00:00,  1.52it/s, v_num=10, train_loss_step=0.558, val_loss=0.639, val_acc=0.778, train_loss_epoch=0.606, train_acc=0.793]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=90` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|██████████| 175/175 [01:55<00:00,  1.52it/s, v_num=10, train_loss_step=0.558, val_loss=0.639, val_acc=0.778, train_loss_epoch=0.606, train_acc=0.793]\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = Path(\"resnet18_finetuned.ckpt\")\n",
    "if ckpt_path.exists():\n",
    "    model = LitResnet18.load_from_checkpoint(ckpt_path)\n",
    "else:\n",
    "    model = LitResnet18()\n",
    "    cifar10_dm = CIFAR10DataModule(num_workers=0)\n",
    "\n",
    "    trainer = L.Trainer(max_epochs=90)\n",
    "    trainer.fit(model, datamodule=cifar10_dm)\n",
    "    trainer.save_checkpoint(ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bdafe",
   "metadata": {},
   "source": [
    "Yes—around 60 cycles is a sensible upper bound if you keep pruning 5 % of the remaining head weights per cycle.\n",
    "\n",
    "Here’s why:\n",
    "\n",
    "With s = 0.05 per cycle, total sparsity after n cycles is\n",
    "\n",
    "1−0.95n\n",
    "1−0.95\n",
    "n\n",
    "\n",
    "So:\n",
    "\n",
    "20 cycles → ~64 % total sparsity\n",
    "\n",
    "40 cycles → ~87 % total sparsity\n",
    "\n",
    "60 cycles → ~95 % total sparsity\n",
    "\n",
    "95 % total sparsity is right at the range where most ResNet-18 heads can no longer recover validation accuracy.\n",
    "Beyond that, recovery is usually negligible and training time is wasted.\n",
    "\n",
    "Going much past 60 cycles gives very small additional pruning (e.g., 80 cycles only brings you to ~98 % sparsity) and very low accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7db80",
   "metadata": {},
   "source": [
    "#### Prune cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6382c62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/aleisley/Documents/ai231/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `lightning.pytorch` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `LitResnet18`",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m prune.l1_unstructured(layer, name=\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m, amount=prune_amount)\n\u001b[32m     13\u001b[39m prune_trainer = L.Trainer(max_epochs=\u001b[32m5\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[43mprune_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcifar10_dm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m acc = prune_trainer.callback_metrics[\u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m].item()\n\u001b[32m     16\u001b[39m results.append({\u001b[33m\"\u001b[39m\u001b[33mcycle\u001b[39m\u001b[33m\"\u001b[39m: cycle, \u001b[33m\"\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m\"\u001b[39m: acc})\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai231/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:553\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\n\u001b[32m    508\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    509\u001b[39m     model: \u001b[33m\"\u001b[39m\u001b[33mpl.LightningModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    513\u001b[39m     ckpt_path: Optional[_PATH] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    514\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    515\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Runs the full optimization routine.\u001b[39;00m\n\u001b[32m    516\u001b[39m \n\u001b[32m    517\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    551\u001b[39m \n\u001b[32m    552\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m     model = \u001b[43m_maybe_unwrap_optimized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    554\u001b[39m     \u001b[38;5;28mself\u001b[39m.strategy._lightning_module = model\n\u001b[32m    555\u001b[39m     _verify_strategy_supports_compile(model, \u001b[38;5;28mself\u001b[39m.strategy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/ai231/.venv/lib/python3.12/site-packages/lightning/pytorch/utilities/compile.py:111\u001b[39m, in \u001b[36m_maybe_unwrap_optimized\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m    109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[32m    110\u001b[39m _check_mixed_imports(model)\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    112\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model).\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    113\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: `model` must be a `LightningModule` or `torch._dynamo.OptimizedModule`, got `LitResnet18`"
     ]
    }
   ],
   "source": [
    "\n",
    "num_cycles = 30\n",
    "prune_amount = 0.10 # 95% sparsity will be reached in 30 cycles\n",
    "\n",
    "layer = model.model.fc\n",
    "results_path = Path('artifacts/pruning_results.json')\n",
    "results = json.loads(results_path.read_text()) if results_path.exists() else []\n",
    "completed = {entry['cycle'] for entry in results}\n",
    "\n",
    "for cycle in range(1, 31):\n",
    "    if cycle in completed:\n",
    "        continue\n",
    "    prune.l1_unstructured(layer, name=\"weight\", amount=prune_amount)\n",
    "    prune_trainer = L.Trainer(max_epochs=5)\n",
    "    prune_trainer.fit(model, datamodule=cifar10_dm)\n",
    "    acc = prune_trainer.callback_metrics[\"val_acc\"].item()\n",
    "    results.append({\"cycle\": cycle, \"val_acc\": acc})\n",
    "    prune_trainer.save_checkpoint(f\"artifacts/cycle_{cycle:.2f}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c999876",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(f'data/resnet18_finetuned2.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62e41288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9576088417247838"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1- 0.9**30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae02dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': tensor(0.8721),\n",
       " 'train_loss_step': tensor(0.7433),\n",
       " 'val_loss': tensor(0.8424),\n",
       " 'val_acc': tensor(0.7294),\n",
       " 'train_loss_epoch': tensor(0.8721),\n",
       " 'train_acc': tensor(0.7277)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd764885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7293999791145325"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callback_metrics[\"val_acc\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cycle in range(num_cycles):\n",
    "    trainer.fit(model, datamodule)\n",
    "    acc = trainer.callback_metrics[\"val_acc\"].item()\n",
    "    results.append({\"cycle\": cycle, \"val_acc\": acc})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai231",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
