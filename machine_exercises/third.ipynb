{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e6da44",
   "metadata": {},
   "source": [
    "### ME3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "de98173e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import lightning as L\n",
    "import torch\n",
    "from einops import einsum\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import ResNet18_Weights, resnet18, ResNet\n",
    "from torch import nn\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torch.nn.utils import prune\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# set precision for matmuls\n",
    "torch.set_float32_matmul_precision(\"medium\")   # or \"medium\"\n",
    "\n",
    "# Resolve storage locations for local runs and Google Drive\n",
    "def resolve_base_dir() -> Path:\n",
    "    env_override = os.environ.get(\"AI231_BASE_DIR\")\n",
    "    if env_override:\n",
    "        return Path(env_override).expanduser()\n",
    "\n",
    "    cwd = Path.cwd()\n",
    "    drive_root = Path(\"/content/drive/MyDrive\")\n",
    "    if drive_root.exists():\n",
    "        try:\n",
    "            cwd.relative_to(drive_root)\n",
    "        except ValueError:\n",
    "            project_dir = drive_root / \"ai231\"\n",
    "            project_dir.mkdir(parents=True, exist_ok=True)\n",
    "            return project_dir\n",
    "        else:\n",
    "            return cwd\n",
    "\n",
    "    return cwd\n",
    "\n",
    "BASE_DIR = resolve_base_dir()\n",
    "DATA_ROOT = BASE_DIR / \"data\" / \"CIFAR10\"\n",
    "ARTIFACT_DIR = BASE_DIR / \"artifacts\"\n",
    "CKPT_PATH = BASE_DIR / \"resnet18_finetuned.ckpt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65ac2ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'bn1.running_mean', 'bn1.running_var', 'bn1.weight', 'bn1.bias', 'layer1.0.conv1.weight', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.conv2.weight', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.1.conv1.weight', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.conv2.weight', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer2.0.conv1.weight', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.conv2.weight', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.1.conv1.weight', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.conv2.weight', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer3.0.conv1.weight', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.conv2.weight', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.1.conv1.weight', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.conv2.weight', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer4.0.conv1.weight', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.conv2.weight', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.1.conv1.weight', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.conv2.weight', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = ResNet18_Weights.DEFAULT.get_state_dict()\n",
    "print(weights_and_biases.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a819b28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0185, -0.0705, -0.0518,  ..., -0.0390,  0.1735, -0.0410],\n",
       "        [-0.0818, -0.0944,  0.0174,  ...,  0.2028, -0.0248,  0.0372],\n",
       "        [-0.0332, -0.0566, -0.0242,  ..., -0.0344, -0.0227,  0.0197],\n",
       "        ...,\n",
       "        [-0.0103,  0.0033, -0.0359,  ..., -0.0279, -0.0115,  0.0128],\n",
       "        [-0.0359, -0.0353, -0.0296,  ..., -0.0330, -0.0110, -0.0513],\n",
       "        [ 0.0021, -0.0248, -0.0829,  ...,  0.0417, -0.0500,  0.0663]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_and_biases['fc.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca15c0c",
   "metadata": {},
   "source": [
    "#### Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "68ed450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_dir: str = \"data/CIFAR10\", \n",
    "            batch_size: int = 256, \n",
    "            num_workers: int = 0\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "    def setup(self, stage: str | None = None):\n",
    "        self.generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "        if stage in (None, 'fit', 'validate'):\n",
    "            cifar10_full = CIFAR10(\n",
    "                root=self.data_dir,\n",
    "                train=True,\n",
    "                transform=self.transform,\n",
    "                download=True,\n",
    "            )\n",
    "            self.cifar10_train, self.cifar10_val = random_split(\n",
    "                cifar10_full, [45000, 5000], generator=self.generator\n",
    "            )\n",
    "        \n",
    "        if stage in (None, \"test\", \"predict\"):\n",
    "            self.cifar10_test = CIFAR10(\n",
    "                root=self.data_dir,\n",
    "                train=False,\n",
    "                transform=self.transform,\n",
    "                download=True,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cifar10_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cifar10_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cifar10_test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=self.num_workers > 0,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b6831",
   "metadata": {},
   "source": [
    "#### Initialize the model and replace head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35adece",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LitResnet18(L.LightningModule):\n",
    "    def __init__(self, num_classes: int = 10, lr: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.model.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        # freeze backbone\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if not name.startswith(\"fc\"):\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return loss, preds, y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self._shared_step(batch, batch_idx)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"train_acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self._shared_step(batch, batch_idx)\n",
    "        self.val_acc.update(preds, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self._shared_step(batch, batch_idx)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"test_acc\", self.test_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        trainable = (p for p in self.parameters() if p.requires_grad)\n",
    "        optimizer = torch.optim.SGD(\n",
    "            trainable, lr=self.hparams.lr, momentum=0.9, weight_decay=1e-4\n",
    "        )\n",
    "        scheduler = MultiStepLR(optimizer, milestones=[30, 60], gamma=0.1)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "# class CIFAR10DataModule(L.LightningDataModule):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         data_dir: Path | str = DATA_ROOT,\n",
    "#         batch_size: int = 256,\n",
    "#         num_workers: int = 4,\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.data_dir = Path(data_dir).expanduser()\n",
    "#         self.batch_size = batch_size\n",
    "#         self.num_workers = num_workers\n",
    "#         self.transform = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "#     def setup(self, stage: str | None = None):\n",
    "#         self.generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "#         if stage in (None, 'fit', 'validate'):\n",
    "#             cifar10_full = CIFAR10(\n",
    "#                 root=self.data_dir,\n",
    "#                 train=True,\n",
    "#                 transform=self.transform,\n",
    "#                 download=True,\n",
    "#             )\n",
    "#             self.cifar10_train, self.cifar10_val = random_split(\n",
    "#                 cifar10_full, [45000, 5000], generator=self.generator\n",
    "#             )\n",
    "\n",
    "#         if stage in (None, \"test\", \"predict\"):\n",
    "#             self.cifar10_test = CIFAR10(\n",
    "#                 root=self.data_dir,\n",
    "#                 train=False,\n",
    "#                 transform=self.transform,\n",
    "#                 download=True,\n",
    "#             )\n",
    "\n",
    "#     def train_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.cifar10_train,\n",
    "#             batch_size=self.batch_size,\n",
    "#             shuffle=True,\n",
    "#             num_workers=self.num_workers,\n",
    "#             persistent_workers=False,\n",
    "#             pin_memory=True,\n",
    "#             drop_last=True,\n",
    "#         )\n",
    "\n",
    "#     def val_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.cifar10_val,\n",
    "#             batch_size=self.batch_size,\n",
    "#             shuffle=False,\n",
    "#             num_workers=0,\n",
    "#             pin_memory=True,\n",
    "#             persistent_workers=False,\n",
    "#         )\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         return DataLoader(\n",
    "#             self.cifar10_test,\n",
    "#             batch_size=self.batch_size,\n",
    "#             shuffle=False,\n",
    "#             num_workers=self.num_workers,\n",
    "#             pin_memory=True,\n",
    "#             persistent_workers=self.num_workers > 0,\n",
    "#         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2fe4aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading resnet18_finetuned.ckpt\n"
     ]
    }
   ],
   "source": [
    "ckpt_path = CKPT_PATH\n",
    "ckpt_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "cifar10_dm = CIFAR10DataModule(num_workers=0)\n",
    "\n",
    "if ckpt_path.exists():\n",
    "    print(f\"Loading {ckpt_path}\")\n",
    "    model = LitResnet18.load_from_checkpoint(ckpt_path)\n",
    "else:\n",
    "    print(\"No checkpoint found; training a new model.\")\n",
    "    model = LitResnet18()\n",
    "    trainer = L.Trainer(max_epochs=90)\n",
    "    trainer.fit(model, datamodule=cifar10_dm)\n",
    "    trainer.save_checkpoint(ckpt_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6bdafe",
   "metadata": {},
   "source": [
    "Yes—around 60 cycles is a sensible upper bound if you keep pruning 5 % of the remaining head weights per cycle.\n",
    "\n",
    "Here’s why:\n",
    "\n",
    "With s = 0.05 per cycle, total sparsity after n cycles is\n",
    "\n",
    "1−0.95n\n",
    "1−0.95\n",
    "n\n",
    "\n",
    "So:\n",
    "\n",
    "20 cycles → ~64 % total sparsity\n",
    "\n",
    "40 cycles → ~87 % total sparsity\n",
    "\n",
    "60 cycles → ~95 % total sparsity\n",
    "\n",
    "95 % total sparsity is right at the range where most ResNet-18 heads can no longer recover validation accuracy.\n",
    "Beyond that, recovery is usually negligible and training time is wasted.\n",
    "\n",
    "Going much past 60 cycles gives very small additional pruning (e.g., 80 cycles only brings you to ~98 % sparsity) and very low accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e7db80",
   "metadata": {},
   "source": [
    "#### Prune cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6382c62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 11.2 M | train\n",
      "1 | loss      | CrossEntropyLoss   | 0      | train\n",
      "2 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "3 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "4 | train_acc | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "72        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/ai231/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/ai231/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  42%|████▏     | 74/175 [00:48<01:06,  1.52it/s, v_num=11, train_loss_step=0.767]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/ai231/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "artifact_dir = ARTIFACT_DIR\n",
    "artifact_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_path = artifact_dir / 'pruning_results.json'\n",
    "\n",
    "num_cycles = 30\n",
    "prune_amount = 0.10  # 95% sparsity will be reached in 30 cycles\n",
    "\n",
    "layer = model.model.fc\n",
    "results = json.loads(results_path.read_text()) if results_path.exists() else []\n",
    "completed = {entry['cycle'] for entry in results}\n",
    "\n",
    "prune_trainer = L.Trainer(max_epochs=5)\n",
    "\n",
    "for cycle in range(1, num_cycles + 1):\n",
    "    if cycle in completed:\n",
    "        continue\n",
    "\n",
    "    prune.l1_unstructured(layer, name=\"weight\", amount=prune_amount)\n",
    "    prune_trainer.fit(model, datamodule=cifar10_dm)\n",
    "    acc = prune_trainer.callback_metrics[\"val_acc\"].item()\n",
    "    results.append({\"cycle\": cycle, \"val_acc\": acc})\n",
    "\n",
    "    ckpt_name = f\"cycle_{cycle:02d}.ckpt\"\n",
    "    prune_trainer.save_checkpoint(artifact_dir / ckpt_name)\n",
    "    results_path.write_text(json.dumps(results, indent=2))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai231",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
