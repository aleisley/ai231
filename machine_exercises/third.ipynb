{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53e6da44",
   "metadata": {},
   "source": [
    "### ME3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de98173e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/ai231/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "\n",
    "# Third-party imports\n",
    "import lightning as L\n",
    "import torch\n",
    "from einops import einsum\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import ResNet18_Weights, resnet18, ResNet\n",
    "from torch import nn\n",
    "from torchmetrics.classification import Accuracy\n",
    "from torch.nn.utils import prune\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# set precision for matmuls\n",
    "torch.set_float32_matmul_precision(\"medium\")   # or \"medium\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65ac2ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['conv1.weight', 'bn1.running_mean', 'bn1.running_var', 'bn1.weight', 'bn1.bias', 'layer1.0.conv1.weight', 'layer1.0.bn1.running_mean', 'layer1.0.bn1.running_var', 'layer1.0.bn1.weight', 'layer1.0.bn1.bias', 'layer1.0.conv2.weight', 'layer1.0.bn2.running_mean', 'layer1.0.bn2.running_var', 'layer1.0.bn2.weight', 'layer1.0.bn2.bias', 'layer1.1.conv1.weight', 'layer1.1.bn1.running_mean', 'layer1.1.bn1.running_var', 'layer1.1.bn1.weight', 'layer1.1.bn1.bias', 'layer1.1.conv2.weight', 'layer1.1.bn2.running_mean', 'layer1.1.bn2.running_var', 'layer1.1.bn2.weight', 'layer1.1.bn2.bias', 'layer2.0.conv1.weight', 'layer2.0.bn1.running_mean', 'layer2.0.bn1.running_var', 'layer2.0.bn1.weight', 'layer2.0.bn1.bias', 'layer2.0.conv2.weight', 'layer2.0.bn2.running_mean', 'layer2.0.bn2.running_var', 'layer2.0.bn2.weight', 'layer2.0.bn2.bias', 'layer2.0.downsample.0.weight', 'layer2.0.downsample.1.running_mean', 'layer2.0.downsample.1.running_var', 'layer2.0.downsample.1.weight', 'layer2.0.downsample.1.bias', 'layer2.1.conv1.weight', 'layer2.1.bn1.running_mean', 'layer2.1.bn1.running_var', 'layer2.1.bn1.weight', 'layer2.1.bn1.bias', 'layer2.1.conv2.weight', 'layer2.1.bn2.running_mean', 'layer2.1.bn2.running_var', 'layer2.1.bn2.weight', 'layer2.1.bn2.bias', 'layer3.0.conv1.weight', 'layer3.0.bn1.running_mean', 'layer3.0.bn1.running_var', 'layer3.0.bn1.weight', 'layer3.0.bn1.bias', 'layer3.0.conv2.weight', 'layer3.0.bn2.running_mean', 'layer3.0.bn2.running_var', 'layer3.0.bn2.weight', 'layer3.0.bn2.bias', 'layer3.0.downsample.0.weight', 'layer3.0.downsample.1.running_mean', 'layer3.0.downsample.1.running_var', 'layer3.0.downsample.1.weight', 'layer3.0.downsample.1.bias', 'layer3.1.conv1.weight', 'layer3.1.bn1.running_mean', 'layer3.1.bn1.running_var', 'layer3.1.bn1.weight', 'layer3.1.bn1.bias', 'layer3.1.conv2.weight', 'layer3.1.bn2.running_mean', 'layer3.1.bn2.running_var', 'layer3.1.bn2.weight', 'layer3.1.bn2.bias', 'layer4.0.conv1.weight', 'layer4.0.bn1.running_mean', 'layer4.0.bn1.running_var', 'layer4.0.bn1.weight', 'layer4.0.bn1.bias', 'layer4.0.conv2.weight', 'layer4.0.bn2.running_mean', 'layer4.0.bn2.running_var', 'layer4.0.bn2.weight', 'layer4.0.bn2.bias', 'layer4.0.downsample.0.weight', 'layer4.0.downsample.1.running_mean', 'layer4.0.downsample.1.running_var', 'layer4.0.downsample.1.weight', 'layer4.0.downsample.1.bias', 'layer4.1.conv1.weight', 'layer4.1.bn1.running_mean', 'layer4.1.bn1.running_var', 'layer4.1.bn1.weight', 'layer4.1.bn1.bias', 'layer4.1.conv2.weight', 'layer4.1.bn2.running_mean', 'layer4.1.bn2.running_var', 'layer4.1.bn2.weight', 'layer4.1.bn2.bias', 'fc.weight', 'fc.bias'])\n"
     ]
    }
   ],
   "source": [
    "weights_and_biases = ResNet18_Weights.DEFAULT.get_state_dict()\n",
    "print(weights_and_biases.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a819b28e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0185, -0.0705, -0.0518,  ..., -0.0390,  0.1735, -0.0410],\n",
       "        [-0.0818, -0.0944,  0.0174,  ...,  0.2028, -0.0248,  0.0372],\n",
       "        [-0.0332, -0.0566, -0.0242,  ..., -0.0344, -0.0227,  0.0197],\n",
       "        ...,\n",
       "        [-0.0103,  0.0033, -0.0359,  ..., -0.0279, -0.0115,  0.0128],\n",
       "        [-0.0359, -0.0353, -0.0296,  ..., -0.0330, -0.0110, -0.0513],\n",
       "        [ 0.0021, -0.0248, -0.0829,  ...,  0.0417, -0.0500,  0.0663]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_and_biases['fc.weight']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca15c0c",
   "metadata": {},
   "source": [
    "#### Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9617017",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "# Build the full training set once, then carve out a validation split\n",
    "full_train = CIFAR10(\n",
    "    root=\"data/CIFAR10\",\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "generator = torch.Generator().manual_seed(42)\n",
    "train_dataset, val_dataset = random_split(full_train, [45000, 5000], generator=generator)\n",
    "\n",
    "cifar10_test = CIFAR10(\n",
    "    root=\"data/CIFAR10\",\n",
    "    train=False,\n",
    "    transform=transform,\n",
    "    download=True,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    cifar10_test,\n",
    "    batch_size=256,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8b6831",
   "metadata": {},
   "source": [
    "#### Initialize the model and replace head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d35adece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as L\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from torchmetrics.classification import Accuracy\n",
    "\n",
    "class LitResnet18(L.LightningModule):\n",
    "    def __init__(self, num_classes: int = 10, lr: float = 1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.model.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "        # freeze backbone\n",
    "        for name, p in self.model.named_parameters():\n",
    "            if not name.startswith(\"fc\"):\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.val_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.train_acc = Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        return loss, preds, y\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self._shared_step(batch, batch_idx)\n",
    "        self.train_acc.update(preds, y)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"train_acc\", self.train_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self._shared_step(batch, batch_idx)\n",
    "        self.val_acc.update(preds, y)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"val_acc\", self.val_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, preds, y = self._shared_step(batch, batch_idx)\n",
    "        self.test_acc.update(preds, y)\n",
    "        self.log(\"test_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "        self.log(\"test_acc\", self.test_acc, on_step=False, on_epoch=True, prog_bar=True, sync_dist=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        trainable = (p for p in self.parameters() if p.requires_grad)\n",
    "        optimizer = torch.optim.SGD(\n",
    "            trainable, lr=self.hparams.lr, momentum=0.9, weight_decay=1e-4\n",
    "        )\n",
    "        scheduler = MultiStepLR(optimizer, milestones=[30, 60], gamma=0.1)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68ed450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10DataModule(L.LightningDataModule):\n",
    "    def __init__(\n",
    "            self, \n",
    "            data_dir: str = \"data/CIFAR10\", \n",
    "            batch_size: int = 256, \n",
    "            num_workers: int = 0\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
    "\n",
    "    def setup(self, stage: str | None = None):\n",
    "        self.generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "        if stage in (None, 'fit', 'validate'):\n",
    "            cifar10_full = CIFAR10(\n",
    "                root=self.data_dir,\n",
    "                train=True,\n",
    "                transform=self.transform,\n",
    "                download=True,\n",
    "            )\n",
    "            self.cifar10_train, self.cifar10_val = random_split(\n",
    "                cifar10_full, [45000, 5000], generator=self.generator\n",
    "            )\n",
    "        \n",
    "        if stage in (None, \"test\", \"predict\"):\n",
    "            self.cifar10_test = CIFAR10(\n",
    "                root=self.data_dir,\n",
    "                train=False,\n",
    "                transform=self.transform,\n",
    "                download=True,\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cifar10_train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=False,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cifar10_val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.cifar10_test,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=self.num_workers > 0,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2fe4aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type               | Params | Mode \n",
      "---------------------------------------------------------\n",
      "0 | model     | ResNet             | 11.2 M | train\n",
      "1 | loss      | CrossEntropyLoss   | 0      | train\n",
      "2 | val_acc   | MulticlassAccuracy | 0      | train\n",
      "3 | test_acc  | MulticlassAccuracy | 0      | train\n",
      "4 | train_acc | MulticlassAccuracy | 0      | train\n",
      "---------------------------------------------------------\n",
      "5.1 K     Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "72        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/ai231/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleisley/Documents/ai231/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175/175 [01:55<00:00,  1.52it/s, v_num=10, train_loss_step=0.558, val_loss=0.639, val_acc=0.778, train_loss_epoch=0.606, train_acc=0.793]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=90` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 175/175 [01:55<00:00,  1.52it/s, v_num=10, train_loss_step=0.558, val_loss=0.639, val_acc=0.778, train_loss_epoch=0.606, train_acc=0.793]\n"
     ]
    }
   ],
   "source": [
    "model = LitResnet18()\n",
    "cifar10_dm = CIFAR10DataModule(num_workers=0)\n",
    "\n",
    "trainer = L.Trainer(max_epochs=90)\n",
    "trainer.fit(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d76dd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(\"resnet18_finetuned.ckpt\")\n",
    "\n",
    "# Reload the module later\n",
    "# model = LitResnet18.load_from_checkpoint(\"resnet18_cifar10.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ae02dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': tensor(0.8721),\n",
       " 'train_loss_step': tensor(0.7433),\n",
       " 'val_loss': tensor(0.8424),\n",
       " 'val_acc': tensor(0.7294),\n",
       " 'train_loss_epoch': tensor(0.8721),\n",
       " 'train_acc': tensor(0.7277)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callback_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd764885",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7293999791145325"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.callback_metrics[\"val_acc\"].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae094254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruning 0.00 of weights\n",
      "Pruning 0.05 of weights\n",
      "Pruning 0.10 of weights\n",
      "Pruning 0.15 of weights\n",
      "Pruning 0.20 of weights\n",
      "Pruning 0.25 of weights\n",
      "Pruning 0.30 of weights\n",
      "Pruning 0.35 of weights\n",
      "Pruning 0.40 of weights\n",
      "Pruning 0.45 of weights\n",
      "Pruning 0.50 of weights\n",
      "Pruning 0.55 of weights\n",
      "Pruning 0.60 of weights\n",
      "Pruning 0.65 of weights\n",
      "Pruning 0.70 of weights\n",
      "Pruning 0.75 of weights\n",
      "Pruning 0.80 of weights\n",
      "Pruning 0.85 of weights\n",
      "Pruning 0.90 of weights\n",
      "Pruning 0.95 of weights\n",
      "Pruning 1.00 of weights\n"
     ]
    }
   ],
   "source": [
    "per_cycle_amount = 0.05\n",
    "cur_cycle_amount = 0.00\n",
    "\n",
    "while cur_cycle_amount < 1.05:\n",
    "    print(f\"Pruning {cur_cycle_amount:.2f} of weights\")\n",
    "    cur_cycle_amount += per_cycle_amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1775e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cycle in range(num_cycles):\n",
    "    trainer.fit(model, datamodule)\n",
    "    acc = trainer.callback_metrics[\"val_acc\"].item()\n",
    "    results.append({\"cycle\": cycle, \"val_acc\": acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fded5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights='IMAGENET1K_V1')\n",
    "model.fc = nn.Linear(in_features=512, out_features=10)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith('fc'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4a52c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone frozen; only fc parameters require gradients.\n"
     ]
    }
   ],
   "source": [
    "non_head_trainable = [name for name, param in model.named_parameters() if param.requires_grad and not name.startswith('fc')]\n",
    "assert not non_head_trainable, f'Non-head parameters still trainable: {non_head_trainable}'\n",
    "print('Backbone frozen; only fc parameters require gradients.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ccaa94f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871a9358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup criterion and optimizer for head-only fine-tuning.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.fc.parameters(),\n",
    "    lr=0.01,\n",
    "    momentum=0.9,\n",
    "    weight_decay=1e-4,\n",
    "    nesterov=True,\n",
    ")\n",
    "\n",
    "print('Setup criterion and optimizer for head-only fine-tuning.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai231",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
